{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6477d440",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs) for Dog Image Generation\n",
    "### Project Overview\n",
    "This project focuses on using Generative Adversarial Networks (GANs) to generate images of dogs. GANs are a class of machine learning models that consist of two neural networks: a generator and a discriminator. The generator creates images, while the discriminator evaluates them, distinguishing between real and generated images. The goal is to train the GAN so that the generated images become indistinguishable from real dog images.\n",
    "\n",
    "### Learning Algorithms and Task\n",
    "- Type of Learning: Deep Learning\n",
    "- Algorithms: Generative Adversarial Networks (GANs)\n",
    "- Task: Image Generation\n",
    "\n",
    "### Motivation and Goal\n",
    "The primary motivation behind this project is to explore the capabilities of GANs in generating realistic images. Specifically, we aim to create images of dogs that can be convincingly classified as real by a pre-trained classifier. This project will not only enhance understanding of GANs but also contribute to the creative frontier of machine learning, where models can generate new and lifelike data.\n",
    "\n",
    "### Data Source and Provenance\n",
    "The dataset used for this project is sourced from the ImageNet database. It includes images of dogs and their corresponding annotations, detailing the breed and bounding box coordinates.\n",
    "\n",
    "### Dataset Citation\n",
    "#### Primary Reference\n",
    "Khosla, A., Jayadevaprakash, N., Yao, B., & Fei-Fei, L. (2011). Stanford Dogs Dataset. Retrieved from http://vision.stanford.edu/aditya86/ImageNetDogs/\n",
    "\n",
    "#### Secondary Reference\n",
    "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image Database. In IEEE Computer Vision and Pattern Recognition (CVPR). Retrieved from http://vision.stanford.edu/aditya86/ImageNetDogs/\n",
    "### Data Description\n",
    "The dataset consists of:\n",
    "- Images: JPG format images of dogs.\n",
    "- Annotations: XML files containing metadata such as image dimensions, dog breed, and bounding box coordinates.\n",
    "### Data Size\n",
    "- Number of Images: 20,580\n",
    "- Annotation Fields: filename, width, height, class, xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829a2b2",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "### Initial Data Loading and Inspection\n",
    "We started by loading the dataset and parsing the annotation files to create a DataFrame. The DataFrame provides a structured view of the annotations associated with each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "images_path = 'all-dogs/'\n",
    "annotations_path = 'Annotation/'\n",
    "\n",
    "# Function to parse annotation files\n",
    "def parse_annotation(annotation_file):\n",
    "    tree = ET.parse(annotation_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    annotation_data = {\n",
    "        'filename': root.find('filename').text,\n",
    "        'width': int(root.find('size/width').text),\n",
    "        'height': int(root.find('size/height').text),\n",
    "        'class': root.find('object/name').text,\n",
    "        'xmin': int(root.find('object/bndbox/xmin').text),\n",
    "        'ymin': int(root.find('object/bndbox/ymin').text),\n",
    "        'xmax': int(root.find('object/bndbox/xmax').text),\n",
    "        'ymax': int(root.find('object/bndbox/ymax').text),\n",
    "    }\n",
    "    \n",
    "    return annotation_data\n",
    "\n",
    "# Load all annotations into a DataFrame\n",
    "annotations = []\n",
    "for annotation_folder in os.listdir(annotations_path):\n",
    "    folder_path = os.path.join(annotations_path, annotation_folder)\n",
    "    for annotation_file in os.listdir(folder_path):\n",
    "        annotation_path = os.path.join(folder_path, annotation_file)\n",
    "        annotation_data = parse_annotation(annotation_path)\n",
    "        annotations.append(annotation_data)\n",
    "\n",
    "annotations_df = pd.DataFrame(annotations)\n",
    "print(annotations_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab43b6a",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "To understand the dataset better, we visualized some sample images with their bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images with bounding boxes\n",
    "def plot_image_with_bounding_box(image_path, annotation):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not read image {image_path}. Skipping...\")\n",
    "        return\n",
    "    cv2.rectangle(image, (annotation['xmin'], annotation['ymin']), (annotation['xmax'], annotation['ymax']), (255, 0, 0), 2)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(annotation['class'])\n",
    "    plt.show()\n",
    "\n",
    "# Plot a few sample images\n",
    "sample_annotations = annotations_df.sample(3)\n",
    "for idx, row in sample_annotations.iterrows():\n",
    "    image_file = row['filename'] + '.jpg'\n",
    "    image_path = os.path.join(images_path, image_file)\n",
    "    plot_image_with_bounding_box(image_path, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ef9e1",
   "metadata": {},
   "source": [
    "### Data Cleaning and Transformation\n",
    "We checked for missing values and ensured that the bounding box coordinates were within the image dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea413905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(annotations_df.isnull().sum())\n",
    "\n",
    "# Ensure bounding box coordinates are within image dimensions\n",
    "annotations_df['xmin'] = annotations_df[['xmin', 'width']].min(axis=1)\n",
    "annotations_df['ymin'] = annotations_df[['ymin', 'height']].min(axis=1)\n",
    "annotations_df['xmax'] = annotations_df[['xmax', 'width']].min(axis=1)\n",
    "annotations_df['ymax'] = annotations_df[['ymax', 'height']].min(axis=1)\n",
    "\n",
    "# Remove any rows with invalid coordinates (optional)\n",
    "annotations_df = annotations_df[(annotations_df['xmin'] >= 0) & (annotations_df['ymin'] >= 0) & \n",
    "                                (annotations_df['xmax'] <= annotations_df['width']) & \n",
    "                                (annotations_df['ymax'] <= annotations_df['height'])]\n",
    "\n",
    "print(annotations_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894acd3d",
   "metadata": {},
   "source": [
    "### Normalization and Preprocessing\n",
    "We normalized the images to have pixel values between 0 and 1, as GANs typically require normalized inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854570c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize images\n",
    "def normalize_image(image):\n",
    "    return image / 255.0\n",
    "\n",
    "# Function to load and preprocess images using annotations\n",
    "def load_and_preprocess_images(images_path, annotations_path):\n",
    "    processed_images = []\n",
    "    annotations = []\n",
    "\n",
    "    for annotation_folder in os.listdir(annotations_path):\n",
    "        folder_path = os.path.join(annotations_path, annotation_folder)\n",
    "        for annotation_file in os.listdir(folder_path):\n",
    "            annotation_path = os.path.join(folder_path, annotation_file)\n",
    "            annotation_data = parse_annotation(annotation_path)\n",
    "            \n",
    "            image_file = annotation_data['filename'] + '.jpg'\n",
    "            image_path = os.path.join(images_path, image_file)\n",
    "            \n",
    "            # Skip if the path contains %s\n",
    "            if '%s' in image_path:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "            if image is not None:\n",
    "                xmin, ymin, xmax, ymax = (annotation_data['xmin'], annotation_data['ymin'],\n",
    "                                          annotation_data['xmax'], annotation_data['ymax'])\n",
    "                w = np.min((xmax - xmin, ymax - ymin))\n",
    "                bbox = (xmin, ymin, xmin + w, ymin + w)\n",
    "                cropped_image = image.crop(bbox)\n",
    "                resized_image = cropped_image.resize((128, 128))\n",
    "                normalized_image = normalize_image(np.array(resized_image))\n",
    "                processed_images.append(normalized_image)\n",
    "                annotations.append(annotation_data)\n",
    "    \n",
    "    return np.array(processed_images), annotations\n",
    "\n",
    "# Display the first image as a preview\n",
    "image_path = os.path.join(images_path, annotations_df.iloc[0]['filename'] + '.jpg')\n",
    "# Load and preprocess all images\n",
    "processed_images, annotations = load_and_preprocess_images(images_path, annotations_path)\n",
    "\n",
    "# Display the first image as a preview\n",
    "if len(processed_images) > 0:\n",
    "    processed_image = processed_images[0]\n",
    "    processed_image_uint8 = (processed_image * 255).astype('uint8')\n",
    "    plt.imshow(cv2.cvtColor(processed_image_uint8, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Preview of the first processed image\")\n",
    "    plt.show()\n",
    "\n",
    "# Print shapes for confirmation\n",
    "print(f'Total images: {len(processed_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11319c4e",
   "metadata": {},
   "source": [
    "## Perform Analysis Using Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU, Conv2D, Conv2DTranspose, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs available:\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"- {gpu}\")\n",
    "else:\n",
    "    print(\"No GPUs available. Training will be performed on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dbb61b",
   "metadata": {},
   "source": [
    "### Basic GAN\n",
    "We'll start with a basic GAN model and gradually move to more advanced architectures.\n",
    "#### Model Architecture\n",
    "The generator is a neural network that takes a random noise vector as input and transforms it into an image. The discriminator is a neural network that takes an image as input and outputs a single value representing whether the image is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAN components\n",
    "def build_generator_gan():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(128 * 128 * 3, activation='tanh'))\n",
    "    model.add(Reshape((128, 128, 3)))\n",
    "    return model\n",
    "\n",
    "def build_discriminator_gan():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(128, 128, 3)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cbc1b1",
   "metadata": {},
   "source": [
    "#### Compile and Train the GAN\n",
    "The GAN is compiled and trained by alternating between training the discriminator and the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train_gan(generator, discriminator, epochs, batch_size, save_interval):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    gan = Sequential([generator, discriminator])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "    train_gan(gan, generator, discriminator, epochs, batch_size, save_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5910ef2",
   "metadata": {},
   "source": [
    "### Deep Convolutional GAN (DCGAN)\n",
    "#### Model Architecture\n",
    "The DCGAN generator uses convolutional layers to create more detailed and high-quality images. The DCGAN discriminator uses convolutional layers to better distinguish between real and fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "def build_generator_dcgan():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 * 16 * 16, activation=\"relu\", input_dim=100))\n",
    "    model.add(Reshape((16, 16, 128)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator_dcgan():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=(128, 128, 3), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff978375",
   "metadata": {},
   "source": [
    "#### Compile and Train the DCGAN\n",
    "The DCGAN is compiled and trained similarly to the basic GAN, but using the DCGAN generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4099ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train_dcgan(generator, discriminator, epochs, batch_size, save_interval):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    gan = Sequential([generator, discriminator])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "    train_gan(gan, generator, discriminator, epochs, batch_size, save_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185079cd",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, generator, discriminator, epochs, batch_size, save_interval):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, processed_images.shape[0], batch_size)\n",
    "        real_imgs = processed_images[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        g_loss = gan.train_on_batch(noise, valid)\n",
    "\n",
    "        if epoch % save_interval == 0 or epoch == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            save_images(generator, epoch)\n",
    "\n",
    "def save_images(generator, epoch, folder='images/'):\n",
    "    noise = np.random.normal(0, 1, (25, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale to [0, 1]\n",
    "\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
    "    count = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axs[i, j].imshow(gen_imgs[count])\n",
    "            axs[i, j].axis('off')\n",
    "            count += 1\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{folder}/gan_generated_image_epoch_{epoch}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d510f",
   "metadata": {},
   "source": [
    "### Train and Compare the Models\n",
    "Train both the basic GAN and DCGAN models, and compare their performance by evaluating the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505afb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the basic GAN\n",
    "generator_gan = build_generator_gan()\n",
    "discriminator_gan = build_discriminator_gan()\n",
    "compile_and_train_gan(generator_gan, discriminator_gan, epochs=20000, batch_size=64, save_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359de04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the DCGAN\n",
    "generator_dcgan = build_generator_dcgan()\n",
    "discriminator_dcgan = build_discriminator_dcgan()\n",
    "compile_and_train_dcgan(generator_dcgan, discriminator_dcgan, epochs=20000, batch_size=64, save_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029b447",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67888658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0002, 0.0001, 0.00005],\n",
    "    'beta1': [0.5, 0.4, 0.6],\n",
    "    'batch_size': [64, 128, 256]\n",
    "}\n",
    "\n",
    "# Random search to select hyperparameters\n",
    "def random_search(param_grid, n_iter=10):\n",
    "    best_params = None\n",
    "    best_performance = float('inf')\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        params = {key: random.choice(values) for key, values in param_grid.items()}\n",
    "        performance = train_gan_with_params(params)\n",
    "        if performance < best_performance:\n",
    "            best_performance = performance\n",
    "            best_params = params\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Train GAN with selected hyperparameters\n",
    "def train_gan_with_params(params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    beta1 = params['beta1']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    generator = build_generator_gan()\n",
    "    discriminator = build_discriminator_gan()\n",
    "    \n",
    "    optimizer = Adam(learning_rate, beta_1=beta1)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan = Sequential([generator, discriminator])\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    train_gan(gan, generator, discriminator, epochs=5000, batch_size=batch_size, save_interval=1000)\n",
    "    \n",
    "    # Dummy performance metric (e.g., loss after a few epochs)\n",
    "    performance = random.random()  # Replace this with actual evaluation metric\n",
    "    return performance\n",
    "\n",
    "# Perform random search\n",
    "best_params = random_search(param_grid, n_iter=10)\n",
    "print(f'Best hyperparameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a40e97",
   "metadata": {},
   "source": [
    "## Analysis Using Deep Learning Models: GAN vs DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af76746",
   "metadata": {},
   "source": [
    "In this analysis, we compare two Generative Adversarial Network (GAN) models: the basic GAN and the Deep Convolutional GAN (DCGAN). The objective is to evaluate which model performs better in generating realistic images of dogs. We will discuss the differences between the models, their architectures, and the outcomes of their training.\n",
    "\n",
    "### Model Architectures\n",
    "#### Basic GAN\n",
    "- Generator: The basic GAN generator uses fully connected (dense) layers to transform a noise vector into a 128x128x3 image.\n",
    "- Discriminator: The discriminator flattens the input image and passes it through several dense layers to classify it as real or fake.\n",
    "DCGAN\n",
    "- Generator: The DCGAN generator uses convolutional transpose layers to upsample the noise vector into a 128x128x3 image. This helps in capturing spatial hierarchies and generating more detailed images.\n",
    "- Discriminator: The DCGAN discriminator uses convolutional layers to downsample the input image, enabling it to better distinguish between real and fake images by capturing local patterns.\n",
    "\n",
    "### Results\n",
    "#### Basic GAN\n",
    "The images generated by the basic GAN were quite blurry and lacked distinct features. The model struggled to create coherent shapes or structures.\n",
    "#### DCGAN\n",
    "The DCGAN generated images with more structure and detail compared to the basic GAN. Visible patterns and textures indicated that the DCGAN was better at capturing and replicating the features of the original dataset.\n",
    "\n",
    "### Discussion\n",
    "The DCGAN outperformed the basic GAN in generating more realistic images. This can be attributed to the following reasons:\n",
    "- Convolutional Layers: DCGAN uses convolutional layers, which are better at capturing spatial hierarchies in images. This helps in generating more detailed and structured outputs.\n",
    "- Batch Normalization: The use of batch normalization in the DCGAN helps in stabilizing the training process and improving the quality of generated images.\n",
    "- Deeper Architecture: The deeper architecture of the DCGAN allows it to model more complex features, resulting in higher-quality images.\n",
    "\n",
    "### Conclusion\n",
    "The DCGAN model demonstrated superior performance in generating realistic images compared to the basic GAN. The use of convolutional layers and batch normalization significantly improved the quality of the generated images. Training both models for more epochs and experimenting with hyperparameters could lead to even better results. This analysis shows the importance of using advanced architectures and techniques in deep learning models to achieve high-quality outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
